{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"runV3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOmAUqKhuWf/DrVyasBq5ez"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5H8XBfhXyxWj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630813777809,"user_tz":-330,"elapsed":421,"user":{"displayName":"bipul kalita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS0iZEhMYgYddyvKNUfu-JvT-Zo8-AeCkGh01Alg=s64","userId":"16142897199248870383"}},"outputId":"bcb2f706-d750-49f8-c2f2-d8a0fe8610bb"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","#before run  encoder sentences must be in descending order\n","loc = '/gdrive/My Drive/Colab Notebooks/seq2seq/v3/'"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"fLzFc6Lt6nQb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630814545898,"user_tz":-330,"elapsed":141476,"user":{"displayName":"bipul kalita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS0iZEhMYgYddyvKNUfu-JvT-Zo8-AeCkGh01Alg=s64","userId":"16142897199248870383"}},"outputId":"7d906b80-b34b-4184-ab51-e71a5743e88c"},"source":["\n","#loader1\n","#import class torch\n","import torch\n","#import class Dataset and DataLoader\n","from torch.utils.data import Dataset,DataLoader\n","#import numpy\n","import numpy as np\n","class load:\n","\tdef __init__(self,encoder,e_max_len,decoder,d_max_len,num_lines,batch_size):\n","\t\tself.e_max_len = e_max_len\n","\t\tself.d_max_len = d_max_len\n","\t\tself.e_data = loader(encoder,e_max_len,num_lines)\n","\t\tself.d_data = loader(decoder,d_max_len,num_lines)\n","\t\n","\t\tself.e\t= DataLoader(dataset=self.e_data,batch_size=batch_size)\n","\t\tself.d\t= DataLoader(dataset=self.d_data,batch_size=batch_size)\n","\n","class loader(Dataset):\n","\tdef __init__(self,file_name,max_length,num_lines):\n","\t\tself.num_lines\t= num_lines\n","\t\tself.max_length\t= max_length\n","\t\tself.file\t= open(file_name).read()\n","\t\tself.vocab\t= { w:i+1 for i,w in enumerate(set(self.file.split()))}\n","\t\tself.sentences  = self.file.split('\\n')\n","\t\tself.data\t= np.zeros([num_lines,max_length])\n","\t\tself.seq_length\t= [len(s.split()) for s in self.sentences if len(s)>0]\n","\t\ti = 0\n","\t\tfor s in self.sentences:\n","\t\t\tif len(s)>0:\n","\t\t\t\tfor j,w in enumerate(s.split()):\n","\t\t\t\t\tself.data[i][j] = self.vocab[w]\n","\t\t\t\ti += 1\n","\t\tself.data\t= torch.from_numpy(self.data).long()\n","\tdef __getitem__(self,index):\n","\t\treturn self.data[index],self.seq_length[index]\n","\tdef __len__(self):\n","\t\treturn self.num_lines\n","\n","\n","\n","#training attention1\n","import torch\n","import pickle\n","embedding_size = 10\n","encoder_file = loc+\"encoder\"\n","decoder_file = loc+\"decoder\"\n","encoder_max_len = 6\n","decoder_max_len = 7\n","num_lines = 14\n","batch_size = 3\n","num_layers = 4\n","ld = load(encoder_file,encoder_max_len,decoder_file,decoder_max_len,num_lines,batch_size)\n","class Attention(torch.nn.Module):\n","\tdef __init__(self,embedding):\n","\t\tsuper(Attention,self).__init__()\n","\t\tself.embedding\t= embedding\n","\t\tself.emb1\t= torch.nn.Embedding(len(ld.e_data.vocab)+1 , self.embedding)\n","\t\tself.lstm1\t= torch.nn.LSTM(num_layers=num_layers,input_size=self.embedding,hidden_size=self.embedding,batch_first=True)\n","\t\tself.lin1\t= torch.nn.Linear(ld.e_max_len,ld.e_max_len)\n","\t\tself.soft\t= torch.nn.Softmax(dim=1)\n","\n","\t\tself.emb2\t= torch.nn.Embedding(len(ld.d_data.vocab)+1,self.embedding,padding_idx=0)\n","\t\tself.lin2\t= torch.nn.Linear(ld.e_max_len+self.embedding,self.embedding)\n","\t\tself.lstm2\t= torch.nn.LSTM(num_layers=num_layers,input_size=self.embedding,hidden_size=self.embedding,batch_first=True)\n","\t\tself.lin3\t= torch.nn.Linear(self.embedding,len(ld.d_data.vocab))\n","\t\t\n","\tdef forward(self,e,d):\n","\t\tloss = 0\n","\t\t#print(d[0])\n","\t\tdx = d[0][:,:-1]\n","\t\t#print(dx)\n","\t\tdy = d[0][:,1:]\n","\t\t#print(dy)\n","\t\tdx = dx.t()\n","\t\t#print(dx)\n","\t\tdy = dy.t()\n","\t\t#print(dy)\n","\t\t\n","\t\tpe = torch.nn.utils.rnn.pack_padded_sequence(e[0],e[1],batch_first=True)\n","\t\t#print(pe)\n","\t\t#print(e)\n","\t\tepe = self.emb1(pe[0])\n","\t\t#print(epe)\n","\t\tepe = torch.nn.utils.rnn.PackedSequence(epe,pe[1])\n","\t\t#print(epe)\n","\t\tepo,(last_hid,last_state) = self.lstm1(epe)\n","\t\t\n","\t\tupo = torch.nn.utils.rnn.pad_packed_sequence(epo,batch_first=True,total_length=ld.e_max_len)[0]\n","\t\t#print(upo)\n","\t\t#print(last_hid)\n","\t\t#------------------------------------------------------------------------------------------#\n","\t\tfor d,l in zip(dx,dy):\n","\t\t\t#modified hidden for multi_layer purpose+++++++++\n","\t\t\tlast_hid_last = last_hid[-1][None]\n","\t\t\t#last_hid_last = last_hid\n","\t\t\n","\t\t\tc = (last_hid_last[-1,:,None]*upo).sum(dim=2)\n","\t\t\t#print(c)\n","\t\t\tc = self.lin1(c)\n","\t\t\t#print(c)\n","\t\t\tc = self.soft(c)\n","\t\t\t#print(c)\n","\t\t\tc = (c[:,:,None]*upo).sum(dim=2)\n","\t\t\t#print(c)\n","\t\t\td = d[:,None]\n","\t\t\t#print(d)\n","\t\t\t#print(l)\n","\t\t\tde = self.emb2(d)\n","\t\t\t#print(de)\n","\t\t\tc = c[:,None]\n","\t\t\t#print(c)\n","\t\t\tde = torch.cat([c,de],dim=2)\n","\t\t\t#print(de)\n","\t\t\tde = self.lin2(de)\n","\t\t\t#print(de)\n","\t\t\tdout,(last_hid,last_state) = self.lstm2(de,(last_hid,last_state))\n","\t\t\t\n","\t\t\t#print(dout)\n","\t\t\t#print(last_hid)\n","\t\t\t#print(dout)\n","\t\t\tnon_zero_l_pos = l.nonzero().view(-1)\n","\t\t\tl = l[non_zero_l_pos]\n","\t\t\tdout = dout[non_zero_l_pos]\n","\t\t\tif len(dout)>0:\n","\t\t\t\tdout = self.lin3(dout)\n","\t\t\t\t#print(dout)\n","\t\t\t\tdout = dout[:,-1,:]\n","\t\t\t\tl    = l-1\n","\t\t\t\t#print(dout)\n","\t\t\t\t#print(l)\n","\t\t\t\tloss += error(dout,l)\n","\t\t\telse:\n","\t\t\t\tcontinue\n","\t\treturn loss\n","\t\t#no problem-----------------------------------------------------------------------------------#\n","\t\t\n","\t\n","\n","\n","attention = Attention(embedding_size)\n","error\t  = torch.nn.CrossEntropyLoss()\n","optim\t  = torch.optim.Adam(attention.parameters(),lr=0.01)\n","for epoch in range(2000):\n","\tfor e,d in zip(ld.e,ld.d):\n","\t\toptim.zero_grad()\n","\t\tloss = attention(e,d)\n","\t\tloss.backward()\n","\t\toptim.step()\n","\t\t#print(loss)\n","print(loss)\n","torch.save(attention.state_dict(),loc+'attention.pt')\n","vocab = {\"e_vocab\":ld.e_data.vocab,\"d_vocab\":ld.d_data.vocab,\"embedding\":10,\"e_max_len\":ld.e_max_len,\"d_max_len\":ld.d_max_len,\"num_layers\":num_layers}\n","print(vocab)\n","f = open(loc+\"vocab\",\"wb\")\n","pickle.dump(vocab,f,pickle.HIGHEST_PROTOCOL)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0009, grad_fn=<AddBackward0>)\n","{'e_vocab': {'help': 1, 'bipul': 2, 'night': 3, 'danger': 4, 'girl': 5, 'limitless': 6, 'is': 7, 'kalita': 8, 'he': 9, 'us': 10, 'life': 11, 'god': 12, 'may': 13, 'not': 14, 'time': 15, 'you': 16, 'ok': 17, 'good': 18, 'a': 19, 'for': 20, 'am': 21, 'she': 22, 'in': 23, 'dark': 24, '</s>': 25, 'i': 26, 'boy': 27, 'name': 28, 'will': 29, 'my': 30, 'oh': 31, 'come': 32}, 'd_vocab': {'xohay': 1, 'bipul': 2, 'moi': 3, 'lora': 4, 'nohoy': 5, 'ejoni': 6, 'kalita': 7, 'aahi': 8, 'karone': 9, 'he': 10, 'naam': 11, 'mor': 12, 'thik': 13, 'endhar': 14, '<s>': 15, 'jibon': 16, 'ximahin': 17, 'amar': 18, 'tai': 19, 'suwali': 20, 'bhogobane': 21, 'korok': 22, 'somoy': 23, 'ejon': 24, 'xi': 25, '</s>': 26, 'apunak': 27, 'bipangat': 28, 'ase': 29, 'bhal': 30, 'raati': 31, 'bhogoban': 32}, 'embedding': 10, 'e_max_len': 6, 'd_max_len': 7, 'num_layers': 4}\n"]}]},{"cell_type":"code","metadata":{"id":"4cS2TI2oxiyb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630814602317,"user_tz":-330,"elapsed":521,"user":{"displayName":"bipul kalita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiS0iZEhMYgYddyvKNUfu-JvT-Zo8-AeCkGh01Alg=s64","userId":"16142897199248870383"}},"outputId":"20bb5aa7-c4ee-4d87-d618-240dcd00197e"},"source":["#result attention\n","import pickle\n","vocab = open(loc+\"vocab\",\"rb\")\n","vocab = pickle.load(vocab)\n","d_vocab = {vocab[\"d_vocab\"][i]:i for i in vocab[\"d_vocab\"]}\n","num_layers = vocab[\"num_layers\"]\n","import torch\n","\n","def get_word(x):\n","\tv,i=torch.max(x,0)\n","\treturn d_vocab[i.item()+1]\n","\t\n","def get_tensor(x):\n","\treturn torch.LongTensor([vocab[\"d_vocab\"][x]])\n","\n","class Attention(torch.nn.Module):\n","\tdef __init__(self,embedding):\n","\t\tsuper(Attention,self).__init__()\n","\t\tself.embedding\t= embedding\n","\t\tself.emb1\t= torch.nn.Embedding(len(vocab[\"e_vocab\"])+1 , self.embedding)\n","\t\tself.lstm1\t= torch.nn.LSTM(num_layers = num_layers,input_size=self.embedding,hidden_size=self.embedding,batch_first=True)\n","\t\tself.lin1\t= torch.nn.Linear(vocab[\"e_max_len\"],vocab[\"e_max_len\"])\n","\t\tself.soft\t= torch.nn.Softmax(dim=1)\n","\n","\t\tself.emb2\t= torch.nn.Embedding(len(vocab[\"d_vocab\"])+1,self.embedding,padding_idx=0)\n","\t\tself.lin2\t= torch.nn.Linear(vocab[\"e_max_len\"]+self.embedding,self.embedding)\n","\t\tself.lstm2\t= torch.nn.LSTM(num_layers = num_layers,input_size=self.embedding,hidden_size=self.embedding,batch_first=True)\n","\t\tself.lin3\t= torch.nn.Linear(self.embedding,len(vocab[\"d_vocab\"]))\n","\t\t\n","\tdef forward(self,e,d):\n","\t\tpe=torch.nn.utils.rnn.pack_padded_sequence(e,[len(e[0])],batch_first=True)\n","\t\te = self.emb1(pe[0])\n","\t\t#print(e)\n","\t\tpe=torch.nn.utils.rnn.PackedSequence(e,pe[1])\n","\t\tepo,(last_hid,last_state) = self.lstm1(pe)\n","\t\t\n","\t\tupo = torch.nn.utils.rnn.pad_packed_sequence(epo,batch_first=True,total_length=vocab[\"e_max_len\"])[0]\n","\t\t#print(upo)\n","\t\t#print(last_hid)\n","\t\tword = \"\"\n","\t\t#++\n","\t\t#d = d[:,None]\n","\t\t#++\n","\t\t#de = self.emb2(d)\n","\t\twhile word!=\"</s>\":\n","\t\t#------------------------------------------------------------------------------------------#\n","\t\t\t#modified hidden for multi_layer purpose+++++++++\n","\t\t\tlast_hid_last = last_hid[-1][None]\n","\t\t\t#last_hid_last = last_hid\n","\t\t\t\n","\t\t\tc = (last_hid_last[-1,:,None]*upo).sum(dim=2)\n","\t\t\tc = self.lin1(c)\n","\t\t\t#print(c)\n","\t\t\tc = self.soft(c)\n","\t\t\t#print(c)\n","\t\t\tc = (c[:,:,None]*upo).sum(dim=2)\n","\t\t\t#print(c)\n","\t\t\t#--\n","\t\t\td = d[:,None]\n","\t\t\t#print(d)\n","\t\t\t#print(l)\n","\t\t\t#--\n","\t\t\tde = self.emb2(d)\n","\t\t\t#print(de)\n","\t\t\tc = c[:,None]\n","\t\t\tde = torch.cat([c,de],dim=2)\n","\t\t\t#print(de)\n","\t\t\tde = self.lin2(de)\n","\t\t\t#print(de)\n","\t\t\tdout,(last_hid,last_state) = self.lstm2(de,(last_hid,last_state))\n","\t\t\t#print(dout)\n","\t\t\t#print(last_hid)\n","\t\t\t#break\n","\t\t\tif len(dout)>0:\n","\t\t\t\tdout = self.lin3(dout)\n","\t\t\t\tdout = dout[:,-1,:]\n","\t\t\t\tword = get_word(dout[0])\n","\t\t\t\tprint(word)\n","\t\t\t\td    = get_tensor(word)\n","\t\t\t\t#++d     = last_hid\n","\t\treturn\t\n","\t\t#no problem-----------------------------------------------------------------------------------#\n","\t\t\n","\t\n","\n","\n","model = Attention(vocab[\"embedding\"])\n","model.load_state_dict(torch.load(loc+'attention.pt'),strict=False)\n","for i in vocab['e_vocab']:\n","\tif(i!='</s>'):\n","\t\tprint(i)\n","\t\tprint('__________')\n","\t\tsentence = i+\" </s>\"\n","\t\tindexing = torch.LongTensor([[vocab[\"e_vocab\"][s] for s in sentence.split()]])\n","\t\td = model(indexing,get_tensor(\"<s>\"))\n","\t\tprint('***************')"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["help\n","__________\n","endhar\n","raati\n","</s>\n","***************\n","bipul\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","night\n","__________\n","endhar\n","</s>\n","***************\n","danger\n","__________\n","somoy\n","raati\n","</s>\n","***************\n","girl\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","limitless\n","__________\n","somoy\n","raati\n","</s>\n","***************\n","is\n","__________\n","thik\n","</s>\n","***************\n","kalita\n","__________\n","somoy\n","kalita\n","</s>\n","***************\n","he\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","us\n","__________\n","endhar\n","raati\n","</s>\n","***************\n","life\n","__________\n","jibon\n","</s>\n","***************\n","god\n","__________\n","bhogoban\n","raati\n","</s>\n","***************\n","may\n","__________\n","mor\n","bipangat\n","</s>\n","***************\n","not\n","__________\n","thik\n","ase\n","</s>\n","***************\n","time\n","__________\n","ximahin\n","</s>\n","***************\n","you\n","__________\n","somoy\n","raati\n","</s>\n","***************\n","ok\n","__________\n","thik\n","ase\n","</s>\n","***************\n","good\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","a\n","__________\n","he\n","bhal\n","</s>\n","***************\n","for\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","am\n","__________\n","somoy\n","kalita\n","</s>\n","***************\n","she\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","in\n","__________\n","endhar\n","raati\n","</s>\n","***************\n","dark\n","__________\n","endhar\n","raati\n","</s>\n","***************\n","i\n","__________\n","somoy\n","raati\n","</s>\n","***************\n","boy\n","__________\n","somoy\n","raati\n","</s>\n","***************\n","name\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","will\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","my\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","oh\n","__________\n","mor\n","xohay\n","</s>\n","***************\n","come\n","__________\n","mor\n","xohay\n","</s>\n","***************\n"]}]},{"cell_type":"code","metadata":{"id":"niGL3_KAM3aO"},"source":["for i in vocab['d_vocab']:\n","  print(i)"],"execution_count":null,"outputs":[]}]}